{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pRJe_3rf8mp0",
        "v7DmN6qn86G2",
        "ZtoZIlGy8-Nu",
        "4SDY-YvI9B-U",
        "5PhzjGs09GKN",
        "raz7YAel9I-_",
        "ehE8TDyKAXHz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Day 5 – Pandas: Data Manipulation II**"
      ],
      "metadata": {
        "id": "20Voy83B8Yzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day 5 of the QuantLake Internship"
      ],
      "metadata": {
        "id": "s5kDOcGT8irl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**"
      ],
      "metadata": {
        "id": "pRJe_3rf8mp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today’s focus was to level up data wrangling skills using Pandas:\n",
        "- Reshape data with `.pivot()`, `.pivot_table()`, and `.melt()`\n",
        "- Apply custom functions with `.apply()`, `.map()`, and `.replace()`\n",
        "- Combine datasets using `pd.concat()`\n",
        "- Build a mini data transformation pipeline using real-world data"
      ],
      "metadata": {
        "id": "QCnyWlig8uM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1: Reshaping DataFrames**"
      ],
      "metadata": {
        "id": "v7DmN6qn86G2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we explored ways to reshape and reorganize datasets using functions like `.pivot()`, `.pivot_table()`, and `.melt()`\n",
        "\n",
        "These methods allow transforming data from long to wide format and vice versa — which is crucial for summarizing or analyzing data across multiple dimensions."
      ],
      "metadata": {
        "id": "UggVU7X5_Yof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mrwsZeyO9boc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "df = pd.read_csv('Sample - Superstore.csv', encoding='cp1252')"
      ],
      "metadata": {
        "id": "GPysnIr39rI6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "data = {\n",
        "    'Region': ['East', 'East', 'West', 'West'],\n",
        "    'Category': ['Furniture', 'Technology', 'Furniture', 'Technology'],\n",
        "    'Sales': [1500, 2500, 1800, 2200]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "tupHIN6H96DF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot\n",
        "pivot_df = df.pivot(index='Region', columns='Category', values='Sales')\n",
        "print(\"Pivot Table:\")\n",
        "print(pivot_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US-tcDyw-NHI",
        "outputId": "2a53731a-6a05-4db3-ebb8-9eacdedfbbc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pivot Table:\n",
            "Category  Furniture  Technology\n",
            "Region                         \n",
            "East           1500        2500\n",
            "West           1800        2200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot Table with aggregation\n",
        "pivot_avg = df.pivot_table(values='Sales', index='Region', columns='Category', aggfunc='mean')\n",
        "print(\"\\nPivot Table with Aggregation:\")\n",
        "print(pivot_avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89WLgjjA-Pl0",
        "outputId": "6de38241-4500-4434-e17d-618fc6ce4b3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pivot Table with Aggregation:\n",
            "Category  Furniture  Technology\n",
            "Region                         \n",
            "East         1500.0      2500.0\n",
            "West         1800.0      2200.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melting (Unpivot)\n",
        "melted = pd.melt(df, id_vars=['Region'], value_vars=['Sales'])\n",
        "print(\"\\nMelted Data:\")\n",
        "print(melted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGxcaqwa-Ryo",
        "outputId": "bf7965db-8f9a-4a3b-8266-460dbd79476a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melted Data:\n",
            "  Region variable  value\n",
            "0   East    Sales   1500\n",
            "1   East    Sales   2500\n",
            "2   West    Sales   1800\n",
            "3   West    Sales   2200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 2: Applying Custom Functions**"
      ],
      "metadata": {
        "id": "ZtoZIlGy8-Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we used `.apply()` with lambda functions to perform row/column-level operations.\n",
        "\n",
        "We created derived columns such as profit classification and flags based on business logic, allowing us to enhance datasets with more insightful and actionable fields."
      ],
      "metadata": {
        "id": "xDfU9mAy_fpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a Profit column\n",
        "df['Profit'] = [300, 500, 200, 700]"
      ],
      "metadata": {
        "id": "ZekkFUcu-UcR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply profit classification\n",
        "df['Profit_Level'] = df['Profit'].apply(lambda x: 'High' if x > 500 else 'Medium' if x > 250 else 'Low')\n",
        "print(\"Profit Level Classification:\")\n",
        "print(df[['Profit', 'Profit_Level']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqwe106K-WXB",
        "outputId": "728d87bf-e90d-42e5-ef73-c9555130f544"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profit Level Classification:\n",
            "   Profit Profit_Level\n",
            "0     300       Medium\n",
            "1     500       Medium\n",
            "2     200          Low\n",
            "3     700         High\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flag high discounts\n",
        "df['Discount'] = [0.1, 0.5, 0.3, 0.92]\n",
        "df['High_Discount'] = df['Discount'].apply(lambda x: 'Yes' if x > 0.9 else 'No')\n",
        "print(\"\\nHigh Discount Flag:\")\n",
        "print(df[['Discount', 'High_Discount']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRNXFRVq-Ye0",
        "outputId": "3cd70c21-4f9d-4520-e4ca-0c9a8e5795cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "High Discount Flag:\n",
            "   Discount High_Discount\n",
            "0      0.10            No\n",
            "1      0.50            No\n",
            "2      0.30            No\n",
            "3      0.92           Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 3: Mapping & Replacing Values**"
      ],
      "metadata": {
        "id": "4SDY-YvI9B-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section focused on cleaning and transforming categorical data using .map() and `.replace()`\n",
        "\n",
        "Typical use cases included mapping full country names to codes or standardizing segment labels, which improves consistency and readability in reports.\n",
        "\n"
      ],
      "metadata": {
        "id": "HjKkilns_kqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping example\n",
        "df['Country'] = ['India', 'USA', 'India', 'Canada']\n",
        "country_map = {'India': 'IN', 'USA': 'US', 'Canada': 'CA'}\n",
        "df['Country_Code'] = df['Country'].map(country_map)"
      ],
      "metadata": {
        "id": "BS8OTPKi-a_I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing example\n",
        "df['Segment'] = ['Consumer', 'Home Office', 'Corporate', 'Consumer']\n",
        "df['Segment'] = df['Segment'].replace({'Consumer': 'Retail'})"
      ],
      "metadata": {
        "id": "ALOegf0G-dPt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mapped & Replaced Columns:\")\n",
        "print(df[['Country', 'Country_Code', 'Segment']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ2FEp2D-fwJ",
        "outputId": "0d9db0d6-0c38-43a5-86cc-696081363d8e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped & Replaced Columns:\n",
            "  Country Country_Code      Segment\n",
            "0   India           IN       Retail\n",
            "1     USA           US  Home Office\n",
            "2   India           IN    Corporate\n",
            "3  Canada           CA       Retail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 4: Combining DataFrames**"
      ],
      "metadata": {
        "id": "5PhzjGs09GKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We practiced merging multiple DataFrames using `pd.concat()` both vertically and horizontally.\n",
        "\n",
        "This is useful for stacking data from different sources or combining features, and also essential in scenarios like appending monthly reports or merging user profiles."
      ],
      "metadata": {
        "id": "ubY8hM-F_ql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame({\n",
        "    'ID': [1, 2],\n",
        "    'Name': ['Alice', 'Bob']\n",
        "})\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'ID': [3, 4],\n",
        "    'Name': ['Charlie', 'Diana']\n",
        "})"
      ],
      "metadata": {
        "id": "zcWssmCw-jWJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vertical concat\n",
        "combined_vert = pd.concat([df1, df2], ignore_index=True)\n",
        "print(\"Vertically Combined:\")\n",
        "print(combined_vert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVKCkmV4-l4_",
        "outputId": "9621a025-fc2f-484d-8e2d-9466578813f1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertically Combined:\n",
            "   ID     Name\n",
            "0   1    Alice\n",
            "1   2      Bob\n",
            "2   3  Charlie\n",
            "3   4    Diana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Horizontal concat with mismatched columns\n",
        "df3 = pd.DataFrame({'ID': [1, 2], 'Age': [23, 30]})\n",
        "horiz_comb = pd.concat([df1, df3], axis=1)\n",
        "print(\"\\nHorizontally Combined:\")\n",
        "print(horiz_comb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LUtKqz9-n6S",
        "outputId": "49fa5dd9-497e-4d43-e829-8655ef8a014c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Horizontally Combined:\n",
            "   ID   Name  ID  Age\n",
            "0   1  Alice   1   23\n",
            "1   2    Bob   2   30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 5: End-to-End Mini Pipeline**"
      ],
      "metadata": {
        "id": "raz7YAel9I-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We built a complete data wrangling pipeline that included loading raw data, filtering, grouping, applying transformations, and pivoting the results to simulate a dashboard-ready summary.\n",
        "\n",
        "This task reflected real-world data transformation steps used in reporting and analytics.\n",
        "\n"
      ],
      "metadata": {
        "id": "oh4A7fvt_v1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "superstore_df = pd.read_csv('Sample - Superstore.csv', encoding='cp1252')"
      ],
      "metadata": {
        "id": "Nv5leDzS-sid"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Filter\n",
        "filtered = superstore_df[superstore_df['Sales'] > 500]"
      ],
      "metadata": {
        "id": "CyRxdjUh-tBS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Add derived column\n",
        "filtered['Profit_Level'] = filtered['Profit'].apply(lambda x: 'High' if x > 100 else 'Low')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X1CS-XY-2TY",
        "outputId": "aa527946-123a-42a2-88d1-b35d59930060"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-24-3514291894.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered['Profit_Level'] = filtered['Profit'].apply(lambda x: 'High' if x > 100 else 'Low')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Group\n",
        "grouped = filtered.groupby(['Category', 'Region'])['Profit'].agg(['sum', 'mean']).reset_index()"
      ],
      "metadata": {
        "id": "9Zlj0hkW-4e3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Pivot\n",
        "pivot_dashboard = grouped.pivot(index='Region', columns='Category', values='mean')\n",
        "print(\"Final Dashboard:\")\n",
        "print(pivot_dashboard)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w24lOyq-6_W",
        "outputId": "9c5f8a83-a167-4c3c-e0b2-08b023d4ebed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Dashboard:\n",
            "Category  Furniture  Office Supplies  Technology\n",
            "Region                                          \n",
            "Central   42.691829       125.734569  297.883231\n",
            "East      27.608349       201.709670  336.405846\n",
            "South     48.965407       139.492102  189.485028\n",
            "West      58.020595       255.967001  251.700606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**"
      ],
      "metadata": {
        "id": "ehE8TDyKAXHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- On Day 5, I learned to reshape, enrich, and transform datasets using advanced Pandas operations.\n",
        "\n",
        "- I applied `.pivot()`, `.pivot_table()`, and `.melt()` for reshaping; used `.apply()` and lambda functions for creating derived columns; cleaned categorical data with `.map()` and `.replace()`, and combined multiple DataFrames using pd `.concat()`.\n",
        "\n",
        "- Finally, I built a mini data processing pipeline — from raw CSV to structured, summarized output — simulating real-world transformation workflows.\n",
        "\n",
        "- These skills form the backbone of efficient data preparation for analytics and dashboards."
      ],
      "metadata": {
        "id": "J7zNJKKEAar_"
      }
    }
  ]
}